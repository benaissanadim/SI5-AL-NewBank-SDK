[[_sink]]
= Sink Connector Guide
:name: Redis Kafka Sink Connector

The {name} consumes records from a Kafka topic and writes the data to Redis.
It includes the following features:

* <<_sink_at_least_once_delivery,At least once delivery>>
* <<_sink_tasks,Multiple tasks>>
* <<_sink_data_structures,Redis Data Structures>>
* <<_sink_data_formats,Supported Data Formats>>

[[_sink_at_least_once_delivery]]
== At least once delivery
The {name} guarantees that records from the Kafka topic are delivered at least once.

[[_sink_tasks]]
== Multiple tasks

The {name} supports running one or more tasks.
You can specify the number of tasks with the `tasks.max` configuration property.

[[_sink_data_structures]]
== Redis Data Structures
The {name} supports the following Redis data-structure types as targets:

[[_collection_key]]
* Collections: <<_sink_stream,stream>>, <<_sink_list,list>>, <<_sink_set,set>>, <<_sink_zset,sorted set>>, <<_sink_timeseries,time series>>
+
Collection keys are generated using the `redis.key` configuration property which may contain `${topic}` (default) as a placeholder for the originating topic name.
+
For example with `redis.key = ${topic}` and topic `orders` the Redis key is `set:orders`.

* <<_sink_hash,Hash>>, <<_sink_string,string>>, <<_sink_json,JSON>>
+
For other data-structures the key is in the form `<keyspace>:<record_key>` where `keyspace` is generated using the `redis.key` configuration property like above and `record_key` is the sink record key.
+
For example with `redis.key = ${topic}`, topic `orders`, and sink record key `123` the Redis key is `orders:123`.

[[_sink_hash]]
=== Hash
Use the following properties to write Kafka records as Redis hashes:

[source,properties]
----
redis.command=HSET
key.converter=<string or bytes> <1>
value.converter=<Avro or JSON> <2>
----

<1> <<_key_string,String>> or <<_key_bytes,bytes>>
<2> <<_avro,Avro>> or <<_kafka_json,JSON>>.
If value is null the key is deleted.

[[_sink_string]]
=== String
Use the following properties to write Kafka records as Redis strings:

[source,properties]
----
redis.command=SET
key.converter=<string or bytes> <1>
value.converter=<string or bytes> <2>
----

<1> <<_key_string,String>> or <<_key_bytes,bytes>>
<2> <<_value_string,String>> or <<_value_bytes,bytes>>.
If value is null the key is deleted.

[[_sink_json]]
=== JSON
Use the following properties to write Kafka records as RedisJSON documents:

[source,properties]
----
redis.command=JSONSET
key.converter=<string, bytes, or Avro> <1>
value.converter=<string, bytes, or Avro> <2>
----

<1> <<_key_string,String>>, <<_key_bytes,bytes>>, or <<_avro,Avro>>
<2> <<_value_string,String>>, <<_value_bytes,bytes>>, or <<_avro,Avro>>.
If value is null the key is deleted.

[[_sink_stream]]
=== Stream
Use the following properties to store Kafka records as Redis stream messages:

[source,properties]
----
redis.command=XADD
redis.key=<stream key> <1>
value.converter=<Avro or JSON> <2>
----

<1> <<_collection_key,Stream key>>
<2> <<_avro,Avro>> or <<_kafka_json,JSON>>

[[_sink_list]]
=== List
Use the following properties to add Kafka record keys to a Redis list:

[source,properties]
----
redis.command=<LPUSH or RPUSH> <1>
redis.key=<key name> <2>
key.converter=<string or bytes> <3>
----

<1> `LPUSH` or `RPUSH`
<2> <<_collection_key,List key>>
<3> <<_key_string,String>> or <<_key_bytes,bytes>>: Kafka record keys to push to the list

The Kafka record value can be any format.
If a value is null then the member is removed from the list (instead of pushed to the list).

[[_sink_set]]
=== Set
Use the following properties to add Kafka record keys to a Redis set:

[source,properties]
----
redis.command=SADD
redis.key=<key name> <1>
key.converter=<string or bytes> <2>
----

<1> <<_collection_key,Set key>>
<2> <<_key_string,String>> or <<_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value can be any format.
If a value is null then the member is removed from the set (instead of added to the set).

[[_sink_zset]]
=== Sorted Set
Use the following properties to add Kafka record keys to a Redis sorted set:

[source,properties]
----
redis.command=ZADD
redis.key=<key name> <1>
key.converter=<string or bytes> <2>
----

<1> <<_collection_key,Sorted set key>>
<2> <<_key_string,String>> or <<_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value should be `float64` and is used for the score.
If the score is null then the member is removed from the sorted set (instead of added to the sorted set).

[[_sink_timeseries]]
=== Time Series
Use the following properties to write Kafka records as RedisTimeSeries samples:

[source,properties]
----
redis.command=TSADD
redis.key=<key name> <1>
----

<1> <<_collection_key,Timeseries key>>

The Kafka record key must be an integer (e.g. `int64`) as it is used for the sample time in milliseconds.

The Kafka record value must be a number (e.g. `float64`) as it is used as the sample value.


[[_sink_data_formats]]
== Data Formats

The {name} supports different data formats for record keys and values depending on the target Redis data structure.

=== Kafka Record Keys
The {name} expects Kafka record keys in a specific format depending on the configured target <<_sink_data_structures,Redis data structure>>:

[options="header",cols="h,1,1"]
|====
|Target|Record Key|Assigned To
|Stream|Any|None
|Hash|String|Key
|String|<<_key_string,String>> or <<_key_bytes,bytes>>|Key
|List|<<_key_string,String>> or <<_key_bytes,bytes>>|Member
|Set|<<_key_string,String>> or <<_key_bytes,bytes>>|Member
|Sorted Set|<<_key_string,String>> or <<_key_bytes,bytes>>|Member
|JSON|<<_key_string,String>> or <<_key_bytes,bytes>>|Key
|TimeSeries|Integer|Sample time in milliseconds
|====

[[_key_string]]
==== StringConverter
If record keys are already serialized as strings use the StringConverter:

[source,properties]
----
key.converter=org.apache.kafka.connect.storage.StringConverter
----

[[_key_bytes]]
==== ByteArrayConverter
Use the byte array converter to use the binary serialized form of the Kafka record keys:

[source,properties]
----
key.converter=org.apache.kafka.connect.converters.ByteArrayConverter
----

=== Kafka Record Values
Multiple data formats are supported for Kafka record values depending on the configured target <<_sink_data_structures,Redis data structure>>.
Each data structure expects a specific format.
If your data in Kafka is not in the format expected for a given data structure, consider using https://docs.confluent.io/platform/current/connect/transforms/overview.html[Single Message Transformations] to convert to a byte array, string, Struct, or map before it is written to Redis.

[options="header",cols="h,1,1"]
|====
|Target|Record Value|Assigned To
|Stream|<<_avro,Avro>> or <<_kafka_json,JSON>>|Message body
|Hash|<<_avro,Avro>> or <<_kafka_json,JSON>>|Fields
|String|<<_value_string,String>> or <<_value_bytes,bytes>>|Value
|List|Any|Removal if null
|Set|Any|Removal if null
|Sorted Set|Number|Score or removal if null
|JSON|<<_value_string,String>> or <<_value_bytes,bytes>>|Value
|TimeSeries|Number|Sample value
|====

[[_value_string]]
==== StringConverter
If record values are already serialized as strings, use the StringConverter to store values in Redis as strings:

[source,properties]
----
value.converter=org.apache.kafka.connect.storage.StringConverter
----

[[_value_bytes]]
==== ByteArrayConverter
Use the byte array converter to store the binary serialized form (for example, JSON, Avro, Strings, etc.) of the Kafka record values in Redis as byte arrays:

[source,properties]
----
value.converter=org.apache.kafka.connect.converters.ByteArrayConverter
----

[[_avro]]
==== Avro
[source,properties]
----
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081
----

[[_kafka_json]]
==== JSON
[source,properties]
----
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=<true|false> <1>
----

<1> Set to `true` if the JSON record structure has an attached schema


