[[_source]]
= Source Connector Guide
:name: Redis Kafka Source Connector

The {name} includes 2 source connectors:

* <<_stream_source,Stream>>
* <<_keys_source,Keys>>

[[_stream_source]]
== Stream

The stream source connector reads from a Redis stream and publishes messages to a Kafka topic.
It includes the following features:

* <<_stream_source_at_least_once_delivery,At least once delivery>>
* <<_stream_source_at_most_once_delivery,At most once delivery>>
* <<_stream_source_tasks,Multiple tasks>>
* <<_stream_source_schema,Schema>>
* <<_stream_source_config,Configuration>>

=== Delivery Guarantees

The {name} can be configured to ack stream messages either automatically (at-most-once delivery) or explicitly (at-least-once delivery).
The default is at-least-once delivery.

[[_stream_source_at_least_once_delivery]]
==== At-Least-Once

In this mode, each stream message is acknowledged after it has been written to the corresponding topic.

[source,properties]
----
redis.stream.delivery=at-least-once
----

[[_stream_source_at_most_once_delivery]]
==== At-Most-Once

In this mode, stream messages are acknowledged as soon as they are read.

[source,properties]
----
redis.stream.delivery=at-most-once
----

[[_stream_source_tasks]]
=== Multiple Tasks
Reading from the stream is done through a consumer group so that multiple instances of the connector configured via the `tasks.max` can consume messages in a round-robin fashion.

[[_stream_source_schema]]
=== Message Schema

==== Key Schema

Keys are of type String and contain the stream message id.

==== Value Schema

The value schema defines the following fields:

[options="header"]
|====
|Name|Schema|Description
|id    |STRING       |Stream message ID
|stream|STRING       |Stream key
|body  |Map of STRING|Stream message body
|====

[[_stream_source_config]]
=== Configuration

[source,properties]
----
connector.class=com.redis.kafka.connect.RedisStreamSourceConnector
redis.stream.name=<name> <1>
redis.stream.offset=<offset> <2>
redis.stream.block=<millis> <3>
redis.stream.consumer.group=<group> <4>
redis.stream.consumer.name=<name> <5>
redis.stream.delivery=<mode> <6>
topic=<name> <7>
----

<1> Name of the stream to read from.
<2> https://redis.io/commands/xread#incomplete-ids[Message ID] to start reading from (default: `0-0`).
<3> Maximum https://redis.io/commands/xread[XREAD] wait duration in milliseconds (default: `100`).
<4> Name of the stream consumer group (default: `kafka-consumer-group`).
<5> Name of the stream consumer (default: `consumer-${task}`).
May contain `${task}` as a placeholder for the task id.
For example, `foo${task}` and task `123` => consumer `foo123`.
<6> Delivery mode: `at-least-once`, `at-most-once` (default: `at-least-once`).
<7> Destination topic (default: `${stream}`).
May contain `${stream}` as a placeholder for the originating stream name.
For example, `redis_${stream}` and stream `orders` => topic `redis_orders`.

[[_keys_source]]
== Keys

The keys source connector captures changes happening to keys in a Redis database and publishes keys and values to a Kafka topic.
The data structure key will be mapped to the record key, and the value will be mapped to the record value.

**Make sure the Redis database has keyspace notifications enabled** using `notify-keyspace-events = KA` in `redis.conf` or via `CONFIG SET`.
For more details see {link_redis_notif}.

[WARNING]
====
The keys source connector does not guarantee data consistency because it relies on Redis keyspace notifications which have no delivery guarantees.
It is possible for some notifications to be missed, for example in case of network failures.

Also, depending on the type, size, and rate of change of data structures on the source it is possible the source connector cannot keep up with the change stream.
For example if a big set is repeatedly updated the connector will need to read the whole set on each update and transfer it over to the target database.
With a big-enough set the connector could fall behind and the internal queue could fill up leading up to updates being dropped.
Some preliminary sizing using Redis statistics and `bigkeys`/`memkeys` is recommended.
If you need assistance please contact your Redis account team.
====

[[_keys_source_config]]
=== Configuration
[source,properties]
----
connector.class=com.redis.kafka.connect.RedisKeysSourceConnector
redis.keys.pattern=<glob> <1>
redis.keys.timeout=<millis> <2>
topic=<name> <3>
----
<1> Key pattern to subscribe to.
This is the key portion of the pattern that will be used to listen to keyspace events.
For example `foo:*` translates to pubsub channel `$$__$$keyspace@0$$__$$:foo:*` and will capture changes to keys `foo:1`, `foo:2`, etc.
See {link_redis_keys} for pattern details.
<2> Idle timeout in millis.
Duration after which the connector will stop if no activity is encountered.
<3> Name of the destination topic.

