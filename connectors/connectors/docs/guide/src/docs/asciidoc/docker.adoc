[[_docker]]
= Quick Start with Docker

This section provides a hands-on look at the functionality of the Redis Kafka Source and Sink Connectors:

* The *redis-sink* connector reads data from a Kafka topic and writes it to a Redis stream
* The *redis-source* connector reads data from a Redis stream and writes it to a Kafka topic

== Requirements

https://docs.docker.com/get-docker/[Docker]

== Run the example

Clone the https://github.com/{github-owner}/{github-repo}.git[{github-repo}] repository and execute `run.sh` in `docker` directory:

[source,console,subs="attributes"]
----
git clone https://github.com/{github-owner}/{github-repo}.git
./run.sh
----

This will:

* Run `docker-compose up`
* Wait for Redis, Kafka, and Kafka Connect to be ready
* Register the Confluent Datagen Connector
* Register the Redis Kafka Sink Connector
* Register the Redis Kafka Source Connector
* Publish some events to Kafka via the Datagen connector
* Write the events to Redis
* Send messages to a Redis stream
* Write the Redis stream messages back into Kafka

Once running, examine the topics in the Kafka control center: http://localhost:9021/

* The `pageviews` topic should contain the 10 simple documents added, each similar to:

[source,json]
----
include::{includedir}/../resources/pageviews.json[]
----

* The `pageviews` stream should contain the 10 change events.

Examine the stream in Redis:
[source,console]
----
docker-compose exec redis /usr/local/bin/redis-cli
xread COUNT 10 STREAMS pageviews 0
----

Messages added to the `mystream` stream will show up in the `mystream` topic


